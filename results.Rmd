```{r summarizing_functions}
source("summarizing.R")
```
```{r search_engine_traffic}
search_engine_traffic_model <- "ARMA(7,5) w/ Gompertz change & regressors"
search_engine_traffic_fit <- readr::read_rds("test/fit_arma75r5ev2.rds")
```
```{r google_desktop_traffic}
google_desktop_traffic_model <- "ARMA(3,2) w/ Gompertz change & regressors"
google_desktop_traffic_fit <- readr::read_rds("fits2/fit_arma32r5ev2.rds")
```

The results presented here are from the best model (see predictive quality in Fig. 4) -- the one with the highest posterior probability of being the model given the data -- with the estimates and intervals in Table 1 based on MCMC samples from the [posterior distribution](https://en.wikipedia.org/wiki/Posterior_probability) of parameters given data. In the model of desktop traffic from Google, the day-of-the-week intercepts $\alpha_1, \ldots, \alpha_7$ shared a mean $\mu_\alpha$, which we can use as an estimate of average daily traffic. Figure 5 shows the inferred effect of the sitemaps over time after they were submitted to the Google Search Console while Table 2 in the Appendix lists the estimates of all parameters of the model. Figure 6 shows the estimates of average desktop traffic from Google by day of the week, and we can see that Sunday and Saturday have the least traffic while Monday has the highest.

```{r posterior_samples}
posterior_samples <- list(
  `All traffic from search engines` = broom::tidyMCMC(search_engine_traffic_fit, pars = c("delta0", "z"), estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.95) %>%
    dplyr::mutate(date = as.Date(c(NA, as.character(itwiki_pvs$date)))),
  `Desktop traffic from Google` = broom::tidyMCMC(google_desktop_traffic_fit, pars = c("delta0", "alpha_mean", "z"), estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.95) %>%
    dplyr::mutate(date = as.Date(c(NA, NA, as.character(google_traffic$date))))
) %>% dplyr::bind_rows(.id = "dataset")
compress <- function(x, ...) {
  y <- polloi::compress(abs(x, ...))
  return(paste0(ifelse(x < 0, "-", ""), y))
}
posterior_samples %>%
  dplyr::filter(
    term %in% c("delta0", "alpha_mean"),
    dataset == "Desktop traffic from Google"
  ) %>%
  dplyr::group_by(term) %>%
  dplyr::summarize(
    estimate = sprintf("%s (%s)", compress(1e6 * estimate), compress(1e6 * std.error)),
    ci95 = sprintf("(%s, %s)", compress(1e6 * conf.low), compress(1e6 * conf.high))
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(term = dplyr::if_else(
    term == "delta0", "$\\delta_0$ (intervention effect)",
    "$\\alpha_\\mu$ (average pageviews per day)"
  )) %>%
  kable(
    booktabs = TRUE, escape = FALSE,
    col.names = c("Parameter", "Point Estimate (Standard Error)", "95\\% Highest Density Interval"),
    caption = "Estimated effect of sitemaps on daily (desktop, not mobile web) pageviews to the Italian Wikipedia from Google. The estimates are the medians calculated from MCMC samples drawn from the posterior distribution of $\\delta_0$ (impact on traffic) and $\\mu_\\alpha$ (average daily traffic). The Highest Density Interval (HDI) contains values such that all values within the interval have a higher probability than points outside the interval."
  ) %>%
  kable_styling(latex_options = c("hold_position"))
```

The effect of the sitemaps ($\delta_0$) has been found to be an additional 744K pageviews (95% CI: 233K--128M), an increase of 42.76% from the daily average of 1.74M. Based on these results, we strongly recommend creating sitemaps for other wikis and submitting them to various search engines' webmaster tools, not just Google Search Console. Although Google accounts for 90% of all search engine-referred traffic,[^seprop] we would still benefit from submitting them to Yahoo (3-4%) and Bing (2-3%) webmaster tools.

[^seprop]: Source: https://discovery.wmflabs.org/external/#traffic_by_engine

While an impact of almost 1M pageviews appears quite impressive, the results warrant further, more rigorous testing. Specifically, the dataset we worked with had two major issues which can be easily overcome in further tests of sitemap impact:

- _**lack of per-engine data**_, which we can overcome by begining to collect per-engine traffic data earlier (this would enable us to more reliably detect the effect of sitemaps on traffic from specific search engines that we submitted the sitemaps to), and
- _**redirect artefact**_, which we did not have time to account for in the model and which will not be an issue for other wikis.

We also fit a similar model to desktop traffic from all recognized search engines, but with an ARMA(7,5) base model of the time series, with months as the random intercepts, and with days of the week as dummy variables in the linear regression component (making their coefficients relative to Sunday). Figure 7 shows the estimates of average traffic by month, with December & August having lowest traffic and January, February, May, and November having highest traffic compared to the other months of the year -- which we saw in Fig. 2. Figure 7 also shows the estimates of average traffic by day of the week -- relative to Sunday (because of how dummy variables in linear regression are required to be specified) -- with lowest traffic on weekends (Saturday much lower than Sunday) and highest traffic on Mondays and Wednesdays.

\clearpage

```{r dataviz_predictions, fig.height=5, fig.width=10, fig.cap="To show the quality of the final model as an explanation of the data, we generated predictions of pageviews for each day to see how the predicted pageview counts matched with observed pageview counts. Most of the time, the actual counts are close to the point estimates or are at least within the prediction interval."}
revelant_events <- events %>%
  dplyr::filter(event %in% c("block start", "GSC submission")) %>%
  dplyr::mutate(event = dplyr::if_else(event == "block start", "redirect", event))
google_desktop_traffic_fit %>%
  posterior_predictive_plot(
    dplyr::select(google_traffic, date, day, actual = pageviews),
    title = "Model-predicted desktop traffic from Google",
    subtitle = google_desktop_traffic_model
  ) +
  scale_x_date(date_labels = "%d %b", date_breaks = "2 weeks", date_minor_breaks = "1 week") +
  geom_vline(aes(xintercept = date), linetype = "dashed", data = revelant_events) +
  geom_label(aes(label = event, y = 3), data = revelant_events) +
  labs(caption = "Actual traffic displayed in black, predictions (estimates and 95% confidence intervals) in red") +
  wmf::theme_min(14)
```

```{r dataviz_change_samples, eval=FALSE}
n_iters <- google_desktop_traffic_fit@stan_args[[1]]$iter / 2 * length(google_desktop_traffic_fit@stan_args)
set.seed(0)
iter_sample <- data.frame(iter = sample.int(n_iters, n_iters * 0.05))
posterior_z <- rstan::extract(google_desktop_traffic_fit, pars = "z")$z %>%
  cbind(iter = 1:n_iters, .) %>%
  dplyr::as_data_frame() %>%
  dplyr::inner_join(iter_sample, by = "iter") %>%
  tidyr::gather(day, z, -iter) %>%
  dplyr::mutate(day = as.numeric(sub("^V", "", day))) %>%
  dplyr::left_join(google_traffic[, c("day", "date")], by = "day")
ggplot(posterior_z) +
  geom_line(aes(x = date, y = z, group = iter), alpha = 0.1)
```
```{r dataviz_change_estimate, fig.height=5, fig.width=10, fig.cap="According to the final model, sitemaps have brought us nearly 744K additional (desktop, not mobile web) pageviews from Google per day in this particular instance."}
additional_CIs <- list(
  ci80 = broom::tidyMCMC(google_desktop_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.80) %>%
    dplyr::mutate(date = google_traffic$date),
  ci50 = broom::tidyMCMC(google_desktop_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.50) %>%
    dplyr::mutate(date = google_traffic$date)
)
posterior_samples %>%
  dplyr::filter(grepl("^z", term), dataset == "Desktop traffic from Google") %>%
  ggplot(aes(x = date)) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci80) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci50) +
  geom_line(aes(y = 1e6 * estimate), size = 1.2) +
  geom_vline(
    aes(xintercept = date), linetype = "dashed",
    data = dplyr::filter(events, event == "GSC submission")
  ) +
  scale_x_date(
    limits = as.Date(c("2018-08-10", "2018-09-17")),
    date_breaks = "7 days", date_minor_breaks = "1 day",
    date_labels = "%b %d"
  ) +
  scale_y_continuous(labels = polloi::compress, breaks = seq(0, 1.4e6, 2e5)) +
  wmf::theme_min(14) +
  labs(
    x = "Date", y = "Pageviews",
    title = "Desktop traffic from Google attributable to sitemaps",
    subtitle = "Day-by-day point estimates with 95%, 80%, 50% confidence intervals"
  )
```

\clearpage

```{r dataviz_weekday_estimates, fig.height=4, fig.width=10, fig.cap="Estimates of weekly seasonality effects as identified by the model fit to desktop traffic to the Italian Wikipedia from Google. Included are the 95% and 50% confidence intervals."}
google_weekdays <- google_desktop_traffic_fit %>%
  rstan::summary(pars = "alpha") %>%
  { .$summary * 1e6 } %>%
  as.data.frame() %>%
  dplyr::mutate(weekday = factor(levels(google_traffic$weekday), levels(google_traffic$weekday)))
ggplot(google_weekdays, aes(x = weekday, y = `50%`)) +
  geom_linerange(aes(ymin = `2.5%`, ymax = `97.5%`), size = 0.4) +
  geom_linerange(aes(ymin = `25%`, ymax = `75%`), size = 1.6, color = "#F0027F") +
  geom_point(size = 2) +
  scale_y_continuous(breaks = seq(0, 4e6, 5e5), labels = compress) +
  labs(
    x = "Day of the week", y = "Pageviews",
    title = "Base desktop traffic from Google, by day of the week",
    subtitle = "As identified by the model"
  ) +
  wmf::theme_min(14)
```

```{r dataviz_seasonality_estimates, fig.width=10, fig.height=6, fig.cap="Estimates of monthly and weekly seasonality effects as identified by the model fit to desktop traffic to the Italian Wikipedia from all recognized search engines. Included are the 95% and 50% confidence intervals."}
alpha_month <- dplyr::data_frame(
  term = paste0("alpha[", 1:12, "]"),
  month = factor(month.name, levels = month.name, labels = month.abb)
)
beta_weekday <- dplyr::data_frame(
  term = paste0("beta[", 2:7, "]"), # beta[1] is the linear trend coefficient 
  weekday = levels(google_traffic$weekday)[-1]
)
search_engine_seasonality <- search_engine_traffic_fit %>%
  rstan::summary(pars = c("alpha", "beta")) %>%
  { .$summary * 1e6 } %>%
  as.data.frame() %>%
  dplyr::mutate(term = rownames(.)) %>%
  dplyr::left_join(alpha_month, by = "term") %>%
  dplyr::left_join(beta_weekday, by = "term") %>%
  dplyr::filter(!(is.na(weekday) & is.na(month))) %>%
  dplyr::bind_rows(dplyr::data_frame(weekday = "Sunday", `50%` = 0)) %>%
  dplyr::mutate(weekday = factor(weekday, levels(google_traffic$weekday)))
p1 <- search_engine_seasonality %>%
  dplyr::filter(!is.na(month)) %>%
  ggplot(aes(x = month, y = `50%`)) +
  geom_linerange(aes(ymin = `2.5%`, ymax = `97.5%`), size = 0.4) +
  geom_linerange(aes(ymin = `25%`, ymax = `75%`), size = 1.6, color = "#F0027F") +
  geom_point(size = 2) +
  scale_y_continuous(labels = compress, breaks = seq(4e5, 2e6, 2e5)) +
  labs(
    y = "Pageviews", x = NULL,
    title = "Base desktop traffic from all recognized search engines",
    subtitle = "Monthly seasonality of traffic as identified by the model"
  ) +
  wmf::theme_min(14)
p2 <- search_engine_seasonality %>%
  dplyr::filter(!is.na(weekday)) %>%
  ggplot(aes(x = weekday, y = `50%`)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_linerange(aes(ymin = `2.5%`, ymax = `97.5%`), size = 0.4) +
  geom_linerange(aes(ymin = `25%`, ymax = `75%`), size = 1.6, color = "#F0027F") +
  geom_point(size = 2) +
  scale_y_continuous(labels = compress, limits = c(-8e5, 8e5), breaks = seq(-8e5, 8e5, 2e5)) +
  scale_x_discrete(limits = rev(c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) +
  labs(
    y = "Fewer \u2190 Pageviews (relative to Sunday) \u2192 More", x = NULL,
    subtitle = "Weekly seasonality of traffic as identified by the model",
    caption = "Estimated day-of-the-week effects are additive to the month intercept, be careful in interpreting this chart"
  ) +
  coord_flip() +
  wmf::theme_min(14) +
  theme(plot.caption = element_text(hjust = 0))
p1 + p2 + plot_layout(ncol = 1)
```
