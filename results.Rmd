```{r summarizing_functions}
source("summarizing.R")
```
```{r search_engine_traffic}
search_engine_traffic_model <- "ARMA(7,5) w/ Gompertz change & regressors"
search_engine_traffic_fit <- readr::read_rds("fits/fit_arma75r5ev2.rds")
```
```{r google_desktop_traffic}
google_desktop_traffic_model <- "ARMA(3,2) w/ Gompertz change & regressors"
google_desktop_traffic_fit <- readr::read_rds("fits/fit_arma32r5ev2.rds")
```

The results presented here are from the best model (see predictive quality in Fig. 4) -- the one with the highest posterior probability of being the model given the data -- with the estimates and intervals in Table 1 based on MCMC samples from the [posterior distribution](https://en.wikipedia.org/wiki/Posterior_probability) of parameters given data. The effect of the sitemaps ($\delta_0$) has been found to be an additional 755K pageviews (95% CI: 180K--1.36M), which would make it an increase of 54% from the estimated average of 1.38M pageviews per day ($\mu$ in the model). Figure 5 shows the inferred effect of the sitemaps over time after they were submitted to the Google Search Console.

```{r posterior_samples}
posterior_samples <- list(
  `All traffic from search engines` = broom::tidyMCMC(search_engine_traffic_fit, pars = c("delta0", "mu", "z"), estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.95) %>%
    dplyr::mutate(date = as.Date(c(NA, NA, as.character(itwiki_pvs$date)))),
  `Desktop traffic from Google` = broom::tidyMCMC(google_desktop_traffic_fit, pars = c("delta0", "mu", "z"), estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.95) %>%
    dplyr::mutate(date = as.Date(c(NA, NA, as.character(google_traffic$date))))
) %>% dplyr::bind_rows(.id = "dataset")
compress <- function(x, ...) {
  y <- polloi::compress(abs(x, ...))
  return(paste0(ifelse(x < 0, "-", ""), y))
}
posterior_samples %>%
  dplyr::filter(term %in% c("delta0", "mu")) %>%
  dplyr::group_by(dataset, term) %>%
  dplyr::summarize(
    estimate = sprintf("%s (%s)", compress(1e6 * estimate), compress(1e6 * std.error)),
    ci95 = sprintf("(%s, %s)", compress(1e6 * conf.low), compress(1e6 * conf.high))
  ) %>%
  dplyr::mutate(term = dplyr::if_else(
    term == "delta0", "$\\delta_0$ (impact)",
    "$\\mu$ (average pageviews per day)"
  )) %>%
  dplyr::ungroup() %>%
  dplyr::select(-dataset) %>%
  kable(
    booktabs = TRUE, escape = FALSE,
    col.names = c("Inferred Parameter", "Point Estimate (Standard Error)", "95\\% Highest Density Interval"),
    caption = "Estimates effect of sitemaps on daily pageviews to the Italian Wikipedia. The estimate is the median calculated from MCMC samples drawn from the posterior distribution of $\\delta_0$. The Highest Density Interval (HDI) contains values such that all values within the interval have a higher probability than points outside the interval."
  ) %>%
  kable_styling(latex_options = c("hold_position")) %>%
  group_rows(index = c("All search engine-referred traffic" = 2, "Google-referred desktop traffic" = 2))
```

Based on these results, we strongly recommend creating sitemaps for other wikis and submitting them to various search engines' webmaster tools, not just Google Search Console. Although Google accounts for 90% of all search engine-referred traffic,[^seprop] we would still benefit from submitting them to Yahoo (3-4%) and Bing (2-3%) webmaster tools.

[^seprop]: Source: https://discovery.wmflabs.org/external/#traffic_by_engine

While an impact of almost 1M pageviews appears quite impressive, the results warrant further, more rigorous testing. Specifically, the dataset we worked with had two major issues which can be easily overcome in further tests of sitemap impact:

- _**lack of per-engine data**_, which we can overcome by begining to collect per-engine traffic data earlier (this would enable us to more reliably detect the effect of sitemaps on traffic from specific search engines that we submitted the sitemaps to), and
- _**redirect artefact**_, which we did not have time to account for in the model and which will not be an issue for other wikis.

\clearpage

```{r dataviz_predictions, fig.height=4.5, fig.width=10, fig.cap="To show the quality of the final model as an explanation of the data, we generated predictions of pageviews for each day to see how the predicted pageview counts matched with observed pageview counts. Most of the time, the actual counts are close to the point estimates or are at least within the prediction interval."}
revelant_events <- events %>%
  dplyr::filter(event %in% c("block start", "GSC submission")) %>%
  dplyr::mutate(event = dplyr::if_else(event == "block start", "redirect", event))
google_desktop_traffic_fit %>%
  posterior_predictive_plot(
    dplyr::select(google_traffic, date, day, actual = pageviews),
    title = "Model-predicted desktop traffic from Google",
    subtitle = google_desktop_traffic_model
  ) +
  scale_x_date(date_labels = "%d %b", date_breaks = "2 weeks", date_minor_breaks = "1 week") +
  geom_vline(aes(xintercept = date), linetype = "dashed", data = revelant_events) +
  geom_label(aes(label = event, y = 3), data = revelant_events) +
  labs(caption = "Actual traffic displayed in black, predictions (estimates and 95% confidence intervals) in red") +
  wmf::theme_min(14)
```

```{r dataviz_change_samples, eval=FALSE}
n_iters <- google_desktop_traffic_fit@stan_args[[1]]$iter / 2 * length(google_desktop_traffic_fit@stan_args)
set.seed(0)
iter_sample <- data.frame(iter = sample.int(n_iters, n_iters * 0.05))
posterior_z <- rstan::extract(google_desktop_traffic_fit, pars = "z")$z %>%
  cbind(iter = 1:n_iters, .) %>%
  dplyr::as_data_frame() %>%
  dplyr::inner_join(iter_sample, by = "iter") %>%
  tidyr::gather(day, z, -iter) %>%
  dplyr::mutate(day = as.numeric(sub("^V", "", day))) %>%
  dplyr::left_join(google_traffic[, c("day", "date")], by = "day")
ggplot(posterior_z) +
  geom_line(aes(x = date, y = z, group = iter), alpha = 0.1)
```
```{r dataviz_change_estimate, fig.height=4.5, fig.width=10, fig.cap="According to the final model, sitemaps have brought us nearly 800K additional (desktop, not mobile web) pageviews from  Google per day in this particular instance."}
additional_CIs <- list(
  ci80 = broom::tidyMCMC(google_desktop_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.80) %>%
    dplyr::mutate(date = google_traffic$date),
  ci50 = broom::tidyMCMC(google_desktop_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.50) %>%
    dplyr::mutate(date = google_traffic$date)
)
posterior_samples %>%
  dplyr::filter(grepl("^z", term), dataset == "Desktop traffic from Google") %>%
  ggplot(aes(x = date)) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci80) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci50) +
  geom_line(aes(y = 1e6 * estimate), size = 1.2) +
  geom_vline(
    aes(xintercept = date), linetype = "dashed",
    data = dplyr::filter(events, event == "GSC submission")
  ) +
  scale_x_date(
    limits = as.Date(c("2018-08-10", "2018-09-17")),
    date_breaks = "7 days", date_minor_breaks = "1 day",
    date_labels = "%b %d"
  ) +
  scale_y_continuous(labels = polloi::compress, breaks = seq(0, 1.4e6, 2e5)) +
  wmf::theme_min(14) +
  labs(
    x = "Date", y = "Pageviews",
    title = "Desktop traffic from Google attributable to sitemaps",
    subtitle = "Day-by-day point estimates with 95%, 80%, 50% confidence intervals"
  )
```
