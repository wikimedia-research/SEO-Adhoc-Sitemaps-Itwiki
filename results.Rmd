```{r summarizing_functions}
source("summarizing.R")
```
```{r search_engine_traffic}
search_engine_traffic_model <- "???"
search_engine_traffic_fit <- readr::read_rds("test/fit_arma32r3ev3.rds")
```

The results presented here are from the best model (see predictive quality in Fig. 4) -- the one with the highest posterior probability of being the model given the data -- with the estimates and intervals in Table 1 based on MCMC samples from the [posterior distribution](https://en.wikipedia.org/wiki/Posterior_probability) of parameters given data. In the model of desktop traffic from search engines, the day-of-the-week intercepts $\alpha_1, \ldots, \alpha_{365}$ shared means $\gamma_1, \ldots, \gamma_{12}$ which in turn shared the mean $\mu_\gamma$, which we can use as an estimate of average daily traffic. Figure 5 shows the extracted effect of the sitemaps over time after they were submitted to the Google Search Console, while Table 2 in the Appendix lists the estimates of all parameters of the model. Figure 6 shows the estimates of average desktop traffic from search engines by month and day of the week, and we can see that... <!-- TODO: check this conclusion -->

```{r posterior_samples}
posterior_samples <- search_engine_traffic_fit %>%
  broom::tidyMCMC(pars = c("delta0", "gamma_mean", "z"), estimate.method = "median",
                  conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.95) %>%
    dplyr::mutate(date = as.Date(c(NA, NA, as.character(itwiki_pvs$date))))
posterior_samples %>%
  dplyr::filter(term %in% c("delta0", "gamma_mean")) %>%
  dplyr::group_by(term) %>%
  dplyr::summarize(
    estimate = sprintf("%s (%s)", compress(1e6 * estimate), compress(1e6 * std.error)),
    ci95 = sprintf("(%s, %s)", compress(1e6 * conf.low), compress(1e6 * conf.high))
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(term = dplyr::if_else(
    term == "delta0", "$\\delta_0$ (intervention effect)",
    "$\\mu_\\gamma$ (average pageviews per day)"
  )) %>%
  kable(
    booktabs = TRUE, escape = FALSE,
    col.names = c("Parameter", "Point Estimate (Standard Error)", "95\\% Highest Density Interval"),
    caption = "Estimated effect of sitemaps on daily (desktop, not mobile web) pageviews to the Italian Wikipedia from search engines. The estimates are the medians calculated from MCMC samples drawn from the posterior distribution of $\\delta_0$ (impact on traffic) and $\\mu_\\gamma$ (average daily traffic). The Highest Density Interval (HDI) contains values such that all values within the interval have a higher probability than points outside the interval."
  ) %>%
  kable_styling(latex_options = c("hold_position"))
```

The effect of the sitemaps ($\delta_0$) has been found to be an additional XX pageviews (95% CI: ??--??), an increase of YY% from the daily average of ZZ. Based on these results, we recommend/don't recommend creating sitemaps for other wikis and submitting them to various search engines' webmaster tools, not just Google Search Console. Although Google accounts for 90% of all search engine-referred traffic[^seprop] and 92-94% in case of the Italian Wikipedia, we would still benefit from submitting them to Yahoo (3-4%) and Bing (2-3%) webmaster tools. <!-- TODO: change this based on results on final model -->

[^seprop]: Source: https://discovery.wmflabs.org/external/#traffic_by_engine

While an impact of almost XX pageviews appears quite impressive, the results warrant further, more rigorous testing. Specifically, the dataset we worked with had one major issue which can be easily overcome in further tests of sitemap impact: we did not have time to account for the redirect artifact in the model and which will not be an issue for other wikis.

\clearpage

```{r dataviz_predictions, fig.height=5, fig.width=10, fig.cap="To show the quality of the final model as an explanation of the data, we generated predictions of pageviews for each day to see how the predicted pageview counts matched with observed pageview counts. Most of the time, the actual counts are close to the point estimates or are at least within the prediction interval."}
revelant_events <- events %>%
  dplyr::filter(event %in% c("block start", "sitemap deployment & GSC submission")) %>%
  dplyr::mutate(event = dplyr::if_else(event == "block start", "redirect", event))
search_engine_traffic_fit %>%
  posterior_predictive_plot(
    dplyr::select(itwiki_pvs, date, day, actual = desktop),
    title = "Model-predicted desktop traffic from search engines",
    subtitle = search_engine_traffic_model
  ) +
  scale_x_date(date_labels = "%d %b", date_breaks = "2 weeks", date_minor_breaks = "1 week", limits = as.Date(c("2018-06-01", "2018-09-25"))) +
  scale_y_continuous(limits = c(0, 4)) +
  geom_vline(aes(xintercept = date), linetype = "dashed", data = revelant_events) +
  geom_label(aes(label = event, y = 0.5), data = revelant_events) +
  labs(caption = "Actual traffic displayed in black, predictions (estimates and 95% confidence intervals) in red") +
  wmf::theme_min(14)
```

```{r dataviz_change_samples, eval=FALSE}
n_iters <- search_engine_traffic_fit@stan_args[[1]]$iter / 2 * length(search_engine_traffic_fit@stan_args)
set.seed(0)
iter_sample <- data.frame(iter = sample.int(n_iters, n_iters * 0.1))
posterior_z <- rstan::extract(search_engine_traffic_fit, pars = "z")$z %>%
  cbind(iter = 1:n_iters, .) %>%
  dplyr::as_data_frame() %>%
  dplyr::inner_join(iter_sample, by = "iter") %>%
  tidyr::gather(day, z, -iter) %>%
  dplyr::mutate(day = as.numeric(sub("^V", "", day))) %>%
  dplyr::left_join(itwiki_pvs[, c("day", "date")], by = "day")
ggplot(posterior_z) +
  geom_line(aes(x = date, y = z, group = iter), alpha = 0.1) +
  scale_x_date(limits = as.Date(c("2018-08-09", "2018-09-25")))
```
```{r dataviz_change_estimate, fig.height=5, fig.width=10, fig.cap="Our model has identified that sitemaps have brought us nearly XXX additional (desktop, not mobile web) pageviews from search engines per day in this particular instance."}
additional_CIs <- list(
  ci80 = broom::tidyMCMC(search_engine_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.80) %>%
    dplyr::mutate(date = itwiki_pvs$date),
  ci50 = broom::tidyMCMC(search_engine_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.50) %>%
    dplyr::mutate(date = itwiki_pvs$date)
)
posterior_samples %>%
  dplyr::filter(grepl("^z", term)) %>%
  ggplot(aes(x = date)) +
  geom_hline(yintercept = 0, linetype = "solid", size = 0.25) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci80) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci50) +
  geom_line(aes(y = 1e6 * estimate), size = 1.2) +
  geom_vline(
    aes(xintercept = date), linetype = "dashed",
    data = dplyr::filter(events, event == "sitemap deployment & GSC submission")
  ) +
  scale_x_date(
    limits = as.Date(c("2018-08-08", "2018-09-25")),
    date_breaks = "7 days", date_minor_breaks = "1 day",
    date_labels = "%b %d"
  ) +
  scale_y_continuous(labels = compress) +
  wmf::theme_min(14) +
  labs(
    x = "Date", y = "Pageviews",
    title = "Desktop traffic from search engines attributable to sitemaps",
    subtitle = "Day-by-day point estimates with 95%, 80%, 50% confidence intervals"
  )
```

\clearpage

```{r dataviz_seasonality_estimates, fig.width=8, fig.height=10, fig.cap="Estimates of monthly and weekly seasonality effects as identified by the model fit to desktop traffic to the Italian Wikipedia from all recognized search engines. Included are the 95% and 50% confidence intervals."}
gamma_month <- dplyr::data_frame(
  term = c("gamma_mean", paste0("gamma[", 1:12, "]")),
  month = factor(c("-", month.name),
                 levels = c("-", month.name),
                 labels = c("Avg", month.abb))
)
beta_weekday <- dplyr::data_frame(
  term = paste0("beta[", 2:7, "]"), # beta[1] is the linear trend coefficient 
  weekday = levels(itwiki_pvs$weekday)[-1]
)
search_engine_seasonality <- search_engine_traffic_fit %>%
  rstan::summary(pars = c("gamma_mean", "gamma", "beta")) %>%
  { .$summary * 1e6 } %>%
  as.data.frame() %>%
  dplyr::mutate(term = rownames(.)) %>%
  dplyr::left_join(gamma_month, by = "term") %>%
  dplyr::left_join(beta_weekday, by = "term") %>%
  dplyr::filter(!(is.na(weekday) & is.na(month))) %>%
  dplyr::bind_rows(dplyr::data_frame(weekday = "Sunday", `50%` = 0)) %>%
  dplyr::mutate(weekday = factor(weekday, levels(itwiki_pvs$weekday)))
p1 <- search_engine_seasonality %>%
  dplyr::filter(!is.na(month)) %>%
  ggplot(aes(x = month, y = `50%`)) +
  geom_linerange(aes(ymin = `2.5%`, ymax = `97.5%`), size = 0.4) +
  geom_linerange(aes(ymin = `25%`, ymax = `75%`), size = 1.6, color = "#F0027F") +
  geom_point(size = 2) +
  scale_y_continuous(labels = compress) +
  labs(
    y = "Pageviews", x = NULL,
    title = "Base desktop traffic from all recognized search engines",
    subtitle = "Monthly seasonality of traffic as identified by the model"
  ) +
  wmf::theme_min(14)
p2 <- search_engine_seasonality %>%
  dplyr::filter(!is.na(weekday)) %>%
  ggplot(aes(x = weekday, y = `50%`)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_linerange(aes(ymin = `2.5%`, ymax = `97.5%`), size = 0.4) +
  geom_linerange(aes(ymin = `25%`, ymax = `75%`), size = 1.6, color = "#F0027F") +
  geom_point(size = 2) +
  scale_y_continuous(labels = compress) +
  scale_x_discrete(limits = rev(c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) +
  labs(
    y = "Fewer \u2190 Pageviews (relative to Sunday) \u2192 More", x = NULL,
    subtitle = "Weekly seasonality of traffic as identified by the model",
    caption = "Estimated day-of-the-week effects are additive to the month intercept, be careful in interpreting this chart"
  ) +
  coord_flip() +
  wmf::theme_min(14) +
  theme(plot.caption = element_text(hjust = 0))
p1 + p2 + plot_layout(ncol = 1)
```
