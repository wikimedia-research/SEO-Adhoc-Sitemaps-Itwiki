```{r summarizing_functions}
source("summarizing.R")
```
```{r search_engine_traffic}
search_engine_traffic_model <- "ARMA(2,1) w/ levelling-off change & regressors (v3)"
search_engine_traffic_fit <- readr::read_rds("final/fit_arma21r3ev3-longer.rds")
```

The results presented here are from the best model (see predictive quality in Fig. 4) -- the one with the highest posterior probability of being the model given the data -- with the estimates and intervals in Table 1 based on MCMC samples from the [posterior distribution](https://en.wikipedia.org/wiki/Posterior_probability) of parameters given data. In the model of desktop traffic from all recognized search engines, the day-of-the-week intercepts $\alpha_1, \ldots, \alpha_{365}$ shared means $\gamma_1, \ldots, \gamma_{12}$ which in turn shared the mean $\mu_\gamma$, which we can use as an estimate of average daily traffic. Figure 5 shows the extracted effect of the sitemaps over time after they were deployed and subsequently submitted to the Google Search Console, while Table 2 in the Appendix lists the estimates of the core parameters of the model, including the long-term effect. Figure 6 shows the estimates of average desktop traffic from search engines by month and day of the week. We can see, for example, that the model confirms February as the month with most traffic, August as the month with the least; and Mondays as the day of the week with most traffic, while Saturdays have significantly less traffic than all other days.

```{r posterior_samples}
posterior_samples <- search_engine_traffic_fit %>%
  broom::tidyMCMC(pars = c("delta0", "limz", "gamma_mean", "z"), estimate.method = "median",
                  conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.95) %>%
    dplyr::mutate(date = as.Date(c(NA, NA, NA, as.character(itwiki_pvs$date))))
percent_change <- posterior_samples %>%
  { .$estimate[.$term == "delta0"] / .$estimate[.$term == "gamma_mean"] } %>%
  { sprintf("%.2f%%", 100 * .) }
estimates <- dplyr::filter(posterior_samples, term %in% c("delta0", "limz", "gamma_mean")) %>%
  dplyr::group_by(term) %>%
  dplyr::summarize(
    estimate = compress(1e6 * estimate),
    ci95 = sprintf("%s--%s", compress(1e6 * conf.low), compress(1e6 * conf.high))
  ) %>%
  unlist
posterior_samples %>%
  dplyr::filter(term %in% c("delta0", "gamma_mean", "limz")) %>%
  dplyr::group_by(term) %>%
  dplyr::summarize(
    estimate = sprintf("%s (%s)", compress(1e6 * estimate), compress(1e6 * std.error)),
    ci95 = sprintf("(%s, %s)", compress(1e6 * conf.low), compress(1e6 * conf.high))
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(term = dplyr::case_when(
    term == "delta0" ~ "$\\delta_0$ (immediate impact)",
    term == "gamma_mean" ~ "$\\mu_\\gamma$ (average pageviews per day)",
    term == "limz" ~ "$\\frac{\\delta_0}{1 - \\omega_1}$ (long-term impact)"
  )) %>%
  kable(
    booktabs = TRUE, escape = FALSE,
    col.names = c("Parameter", "Point Estimate (Standard Error)", "95\\% Highest Density Interval"),
    caption = "Estimated effect of sitemaps on daily (desktop, not mobile web) pageviews to the Italian Wikipedia from search engines. The estimates are the medians calculated from MCMC samples drawn from the posterior distribution of $\\delta_0$ (impact on traffic) and $\\mu_\\gamma$ (average daily traffic). The Highest Density Interval (HDI) contains values such that all values within the interval have a higher probability than points outside the interval."
  ) %>%
  kable_styling(latex_options = c("hold_position"))
```

The immediate effect of the sitemaps ($\delta_0$) has been found to be an additional `r estimates['estimate1']` pageviews (95% CI: `r estimates['ci951']`), an increase of `r percent_change` from the daily average of `r estimates['estimate2']` The long-term impact (see Methods section) is estimated to be `r estimates['estimate3']` (95% CI: `r estimates['ci953']`) pageviews per day. Based on these results, we recommend verifying with additional tests by creating/deploying sitemaps for a couple of other wikis -- while taking care to make sure there are no additional interventions which might leave an artifact in the dataset the way the Italian Wikipedia redirect has -- and submitting them to various search engines' webmaster tools, not just Google Search Console. Although Google accounts for 90% of all search engine-referred traffic[^seprop] and 92-94% in case of the Italian Wikipedia, we could still benefit from submitting them to Yahoo (3-4%) and Bing (2-3%) webmaster tools.

[^seprop]: Source: https://discovery.wmflabs.org/external/#traffic_by_engine

# Acknowledgements

We would like to give a special thanks to [Tilman Bayer](https://wikimediafoundation.org/profile/tilman-bayer/), whose feedback and criticism immensely aided the analysis presented in this paper. We would also like to acknowledge the work of the [Wikimedia Cloud Services Team](https://wikitech.wikimedia.org/wiki/Help:Cloud_Services_Introduction), as much of the computation was done on a virtual machine hosted by [Cloud VPS](https://wikitech.wikimedia.org/wiki/Portal:Cloud_VPS).

\clearpage

```{r dataviz_predictions, fig.height=5, fig.width=10, fig.cap="To show the quality of the final model as an explanation of the data, we generated predictions of pageviews for each day to see how the predicted pageview counts matched with observed pageview counts. Most of the time, the actual counts are close to the point estimates or are at least within the prediction interval."}
revelant_events <- events %>%
  dplyr::filter(event %in% c("block start", "sitemap deployment & GSC submission")) %>%
  dplyr::mutate(event = dplyr::if_else(event == "block start", "redirect", event))
search_engine_traffic_fit %>%
  posterior_predictive_plot(
    dplyr::select(itwiki_pvs, date, day, actual = desktop),
    title = "Model-predicted desktop traffic from search engines",
    subtitle = search_engine_traffic_model
  ) +
  scale_x_date(date_labels = "%d %b", date_breaks = "2 weeks", date_minor_breaks = "1 week", limits = as.Date(c("2018-06-01", "2018-09-25"))) +
  scale_y_continuous(limits = c(0, 4)) +
  geom_vline(aes(xintercept = date), linetype = "dashed", data = revelant_events) +
  geom_label(aes(label = event, y = 0.5), data = revelant_events) +
  labs(caption = "Actual traffic displayed in black, predictions (estimates and 95% confidence intervals) in red") +
  wmf::theme_min(14)
```

```{r dataviz_change_samples, eval=FALSE}
n_iters <- search_engine_traffic_fit@stan_args[[1]]$iter / 2 * length(search_engine_traffic_fit@stan_args)
set.seed(0)
iter_sample <- data.frame(iter = sample.int(n_iters, n_iters * 0.1))
posterior_z <- rstan::extract(search_engine_traffic_fit, pars = "z")$z %>%
  cbind(iter = 1:n_iters, .) %>%
  dplyr::as_data_frame() %>%
  dplyr::inner_join(iter_sample, by = "iter") %>%
  tidyr::gather(day, z, -iter) %>%
  dplyr::mutate(day = as.numeric(sub("^V", "", day))) %>%
  dplyr::left_join(itwiki_pvs[, c("day", "date")], by = "day")
ggplot(posterior_z) +
  geom_line(aes(x = date, y = z, group = iter), alpha = 0.1) +
  scale_x_date(limits = as.Date(c("2018-08-09", "2018-09-25")))
```
```{r dataviz_change_estimate, fig.height=5, fig.width=10, fig.cap="Our model has identified that sitemaps have brought us nearly 9.25K immediate (35K long-term) additional (desktop, not mobile web) pageviews from search engines per day in this particular instance."}
additional_CIs <- list(
  ci80 = broom::tidyMCMC(search_engine_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.80) %>%
    dplyr::mutate(date = itwiki_pvs$date),
  ci50 = broom::tidyMCMC(search_engine_traffic_fit, pars = "z", estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval", conf.level = 0.50) %>%
    dplyr::mutate(date = itwiki_pvs$date)
)
posterior_samples %>%
  dplyr::filter(grepl("^z", term)) %>%
  ggplot(aes(x = date)) +
  geom_hline(yintercept = 0, linetype = "solid", size = 0.25) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci80) +
  geom_ribbon(aes(ymin = 1e6 * conf.low, ymax = 1e6 * conf.high), alpha = 0.2, data = additional_CIs$ci50) +
  geom_line(aes(y = 1e6 * estimate), size = 1.2) +
  geom_vline(
    aes(xintercept = date), linetype = "dashed",
    data = dplyr::filter(events, event == "sitemap deployment & GSC submission")
  ) +
  scale_x_date(
    limits = as.Date(c("2018-08-08", "2018-09-25")),
    date_breaks = "7 days", date_minor_breaks = "1 day",
    date_labels = "%b %d"
  ) +
  scale_y_continuous(labels = compress) +
  wmf::theme_min(14) +
  labs(
    x = "Date", y = "Pageviews",
    title = "Desktop traffic from search engines attributable to sitemaps",
    subtitle = "Day-by-day point estimates with 95%, 80%, 50% confidence intervals"
  )
```

\clearpage

```{r dataviz_seasonality_estimates, fig.width=8, fig.height=10, fig.cap="Estimates of monthly and weekly seasonality effects as identified by the model fit to desktop traffic to the Italian Wikipedia from all recognized search engines. Included are the 95% and 50% confidence intervals."}
gamma_month <- dplyr::data_frame(
  term = c("gamma_mean", paste0("gamma[", 1:12, "]")),
  month = factor(c("-", month.name),
                 levels = c("-", month.name),
                 labels = c("Avg", month.abb))
)
beta_weekday <- dplyr::data_frame(
  term = paste0("beta[", 2:7, "]"), # beta[1] is the linear trend coefficient 
  weekday = levels(itwiki_pvs$weekday)[-1]
)
search_engine_seasonality <- search_engine_traffic_fit %>%
  rstan::summary(pars = c("gamma_mean", "gamma", "beta")) %>%
  { .$summary * 1e6 } %>%
  as.data.frame() %>%
  dplyr::mutate(term = rownames(.)) %>%
  dplyr::left_join(gamma_month, by = "term") %>%
  dplyr::left_join(beta_weekday, by = "term") %>%
  dplyr::filter(!(is.na(weekday) & is.na(month))) %>%
  dplyr::bind_rows(dplyr::data_frame(weekday = "Sunday", `50%` = 0)) %>%
  dplyr::mutate(weekday = factor(weekday, levels(itwiki_pvs$weekday)))
p1 <- search_engine_seasonality %>%
  dplyr::filter(!is.na(month)) %>%
  ggplot(aes(x = month, y = `50%`)) +
  geom_linerange(aes(ymin = `2.5%`, ymax = `97.5%`), size = 0.4) +
  geom_linerange(aes(ymin = `25%`, ymax = `75%`), size = 1.6, color = "#F0027F") +
  geom_point(size = 2) +
  scale_y_continuous(labels = compress) +
  labs(
    y = "Pageviews", x = NULL,
    title = "Base desktop traffic from all recognized search engines",
    subtitle = "Monthly seasonality of traffic as identified by the model"
  ) +
  wmf::theme_min(14)
p2 <- search_engine_seasonality %>%
  dplyr::filter(!is.na(weekday)) %>%
  ggplot(aes(x = weekday, y = `50%`)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_linerange(aes(ymin = `2.5%`, ymax = `97.5%`), size = 0.4) +
  geom_linerange(aes(ymin = `25%`, ymax = `75%`), size = 1.6, color = "#F0027F") +
  geom_point(size = 2) +
  scale_y_continuous(labels = compress) +
  scale_x_discrete(limits = rev(c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) +
  labs(
    y = "Fewer \u2190 Pageviews (relative to Sunday) \u2192 More", x = NULL,
    subtitle = "Weekly seasonality of traffic as identified by the model",
    caption = "Estimated day-of-the-week effects are additive to the month intercept, be careful in interpreting this chart"
  ) +
  coord_flip() +
  wmf::theme_min(14) +
  theme(plot.caption = element_text(hjust = 0))
p1 + p2 + plot_layout(ncol = 1)
```

\clearpage

\blandscape

```{r dataviz_seasonality_annual, fig.width=10, fig.height=6, fig.cap="Estimates and confidence intervals from fitting the model with the daily, hierarchical random intercepts to search engine-referred desktop traffic, shown in black. Since each day in a given month is centered around the mean for that month, the estimates and confidence intervals of the month means are included and shown in pink.", out.extra=""}
dates <- data.frame(date = seq(as.Date("2018-01-01"), as.Date("2018-12-31"), by = "day")) %>%
  dplyr::mutate(yd = lubridate::yday(date), month = lubridate::month(date))
month_marks <- dates %>%
  dplyr::mutate(md = lubridate::mday(date)) %>%
  dplyr::filter(md == 1)
monthly_seasonality <- broom::tidyMCMC(
  search_engine_traffic_fit, pars = "gamma",
  estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval"
) %>%
  dplyr::mutate(month = as.numeric(sub("gamma\\[([0-9]{1,2})\\]", "\\1", term))) %>%
  dplyr::left_join(dates, ., by = "month")
annual_seasonality <- broom::tidyMCMC(
  search_engine_traffic_fit, pars = "alpha",
  estimate.method = "median", conf.int = TRUE, conf.method = "HPDinterval"
) %>%
  dplyr::mutate(yd = as.numeric(sub("alpha\\[([0-9]{1,3})\\]", "\\1", term))) %>%
  dplyr::left_join(dates, by = "yd")
monthly_seasonality %>%
  ggplot(aes(x = date, y = 1e6 * estimate, ymin = 1e6 * conf.low, ymax = 1e6 * conf.high)) +
  geom_vline(aes(xintercept = date), data = month_marks, linetype = "dashed", size = 0.25) +
  geom_ribbon(alpha = 0.1, fill = "#F0027F") +
  geom_line(color = "#F0027F", size = 1.1) +
  geom_ribbon(data = annual_seasonality, alpha = 0.3, fill = "#7570b3") +
  geom_line(data = annual_seasonality) +
  scale_y_continuous(labels = compress) +
  scale_x_date(date_breaks = "1 month", date_labels = "%d\n%b") +
  labs(
    x = "Day of year", y = "Pageviews",
    title = "Average traffic to the Italian Wikipedia from all recognized search engines",
    subtitle = "Per-day estimates from data collected since February 2016, with per-month estimates in pink"
  ) +
  wmf::theme_min(14)
```

\elandscape
